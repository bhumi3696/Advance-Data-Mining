my_dataset$JobLevel <- factor(my_dataset$JobLevel) #insufficient information to do more
my_dataset$JobSatisfaction <- factor(my_dataset$JobSatisfaction, levels = c(1:4), labels=c("Low", "Medium", "High", "v High"))
my_dataset$PerformanceRating <- factor(my_dataset$PerformanceRating, levels = c(1:4), labels=c("Low", "Good", "Excellent", "Outstanding"))
my_dataset$RelationshipSatisfaction <- factor(my_dataset$RelationshipSatisfaction, levels = c(1:4), labels=c("Low", "Medium", "High", "v High"))
my_dataset$StockOptionLevel <- factor(my_dataset$StockOptionLevel) #don't have more information
my_dataset$WorkLifeBalance <- factor(my_dataset$WorkLifeBalance, levels = c(1:4), labels=c("Bad", "Good", "Better", "Best"))
#Now to fix factors where i don't have all levels
summary(my_dataset)
#I'm missing levels 0 and 1 in PerformanceRating, but the rest of my categoricals seem fine
my_dataset$PerformanceRating <- factor(my_dataset$PerformanceRating)
#however, as JobRole displays other, let's check more precisely
table(my_dataset$JobRole)
#because i already encoded them above, all i have to do is reinvoke the factor command and my empty levels disappear
#a couple of the variables are pointless, so let's get rid of them
my_dataset <- my_dataset[,-9] #EmployeeCount -- always 1
my_dataset <- my_dataset[, -19] #over 18 -- always Y; strictly speaking we should confirm this, but under time pressure it's safe to assume this varibale isn't needed
#mising values:
sapply(my_dataset,function(x) sum(is.na(x)))
#outliers
boxplot(subset(my_dataset, select=c(1,6,18,20,21:24))) #nothing too obvious here
boxplot(subset(my_dataset, select=c(26:29))) #May be something in years at company
boxplot(my_dataset$DailyRate)
boxplot(my_dataset$HourlyRate)
boxplot(my_dataset$MonthlyIncome) #possibly some here too
boxplot(my_dataset$MonthlyRate)
#i prefer the attrition factor as 0/1 for convenience
my_dataset$Attrition <- factor(my_dataset$Attrition, labels=c(0,1), levels=c("No", "Yes"))
#F2: class balance:
table(my_dataset$Attrition) #fairly imbalanced
#F3: train and test
#2 options here stratified, or under/over sampling
library(caret)
install.packages('Rcpp')
#F3: train and test
#2 options here stratified, or under/over sampling
library(caret)
install.packages('colorspace')
#F3: train and test
#2 options here stratified, or under/over sampling
library(caret)
install.packages('lazyeval')
#F3: train and test
#2 options here stratified, or under/over sampling
library(caret)
install.packages('rlang')
#F3: train and test
#2 options here stratified, or under/over sampling
library(caret)
install.packages('tibble')
#F3: train and test
#2 options here stratified, or under/over sampling
library(caret)
#F3: train and test
#2 options here stratified, or under/over sampling
library(caret)
install.packages('glue')
#F3: train and test
#2 options here stratified, or under/over sampling
library(caret)
install.packages('purrr')
#F3: train and test
#2 options here stratified, or under/over sampling
library(caret)
install.packages('gower')
#F3: train and test
#2 options here stratified, or under/over sampling
library(caret)
install.packages('data.table')
#F3: train and test
#2 options here stratified, or under/over sampling
library(caret)
sample <- createDataPartition(my_dataset$Attrition, p = .75, list = FALSE)
train <- my_dataset[sample, ]
test <- my_dataset[-sample, ]
#F4: benchmarks
#1 everyone stays:
str(test$Attrition) # remain (Y) is 0
b1 <- rep(0, dim(test)[1])
(accuracyB1 <- 1 - mean(b1 != test$Attrition))
#here we really see the effect of the class imbalance!
#2 those who have higher satisfaction are less likely to leave
str(my_dataset$JobSatisfaction)
b2 <- rep(0, dim(test)[1])
b2[test$JobSatisfaction == 'Low'] <- 1
b2[test$JobSatisfaction == 'Medium'] <- 1
(accuracyB2 <- 1 - mean(b2 != test$Attrition))
summary(test$JobSatisfaction)
str(subset(my_dataset, select=c(1,4,6,10,16:19,23,24,26:29)))
#now correlation test
cor(subset(my_dataset, select=c(1,4,6,10,16:19,23,24,26:29)))
boxplot(my_dataset$Age ~ my_dataset$Attrition)
spineplot(my_dataset$Education, my_dataset$Attrition)
spineplot(my_dataset$MaritalStatus, my_dataset$Attrition) #single people seem to leave more often
boxplot(my_dataset$DistanceFromHome ~ my_dataset$MaritalStatus) #surprisingly there doesn't appear to be much difference in commute distance by martial status
#Start of Question: B4
plot(my_dataset$Age, my_dataset$MonthlyRate, main = "Scatter plot of Age vs. Monthly Rate")
plot(my_dataset$Age, my_dataset$DailyRate, main = "Scatter plot of Age vs. Daily Rate")
plot(my_dataset$Age, my_dataset$HourlyRate, main = "Scatter plot of Age vs. Hourly Rate")
#Start of Question: B5
(counts <- table(my_dataset$Attrition, my_dataset$JobSatisfaction))
row.names(counts) <- c("Remained", "Left")
barplot(counts, main="Attrition Distribution by Job Satisfaction", legend = row.names(counts))
#or if you want something a little easier to read
barplot(counts, main="Attrition Distribution by Job Satisfaction", xlab="Job Satistfaction", col=c("darkblue","red"), legend = rownames(counts), beside=T)
library(C50)
cFifty <- C5.0(Attrition ~ ., data=train, trials=100)
cFiftyPrediction <- predict(cFifty, newdata = test[, -2])
#Start of Question: B2
boxplot(my_dataset$ï..Age ~ my_dataset$Attrition)
spineplot(my_dataset$Education, my_dataset$Attrition)
#Start of Question: B3
#I don't have gender, or overtime, so selecting marital status
spineplot(my_dataset$MaritalStatus, my_dataset$Attrition) #single people seem to leave more often
boxplot(my_dataset$DistanceFromHome ~ my_dataset$MaritalStatus) #surprisingly there doesn't appear to be much difference in commute distance by martial status
#Start of Question: B4
plot(my_dataset$Age, my_dataset$MonthlyRate, main = "Scatter plot of Age vs. Monthly Rate")
#Start of Question: B4
plot(my_dataset$ï..Age, my_dataset$MonthlyRate, main = "Scatter plot of Age vs. Monthly Rate")
plot(my_dataset$ï..Age, my_dataset$DailyRate, main = "Scatter plot of Age vs. Daily Rate")
plot(my_dataset$ï..Age, my_dataset$HourlyRate, main = "Scatter plot of Age vs. Hourly Rate")
#Start of Question: B5
(counts <- table(my_dataset$Attrition, my_dataset$JobSatisfaction))
row.names(counts) <- c("Remained", "Left")
barplot(counts, main="Attrition Distribution by Job Satisfaction", legend = row.names(counts))
#or if you want something a little easier to read
barplot(counts, main="Attrition Distribution by Job Satisfaction", xlab="Job Satistfaction", col=c("darkblue","red"), legend = rownames(counts), beside=T)
library(C50)
install.packages('libcoin')
library(C50)
library(C50)
cFifty <- C5.0(Attrition ~ ., data=train, trials=100)
cFiftyPrediction <- predict(cFifty, newdata = test[, -2])
(cFiftyAccuracy <- 1- mean(cFiftyPrediction != test$Attrition))
cFiftyAccuracy - accuracyB1
cFiftyAccuracy - accuracyB2
library(class)
normalize <- function(x) {return ((x - min(x)) / (max(x) - min(x)))}
#now let's normalise our dataset so that calculating the distances in the feature space makes sense
my_dataset_n <- subset(my_dataset, select=c(1,4,6,10,16:19,23,24,26:29)) #get the numerical for normalisation -- kNN also doesn't support levelled factors either
my_dataset_n <- as.data.frame(lapply(my_dataset_n, normalize)) #normalise
summary(my_dataset_n) #all our numericals are normalised, our categoricals are untouched
#re make train and test note we can retain the original distribution if we choose to
train_n <- my_dataset_n[sample, ]
test_n <- my_dataset_n[-sample, ]
#different ways to determine k
k1 <- round(sqrt(dim(train_n)[1])) #sqrt of number of instances
k2 <- round(sqrt(dim(train_n)[2])) #sqrt of number of attributes
k3 <- 7 #a number between 3 and 10
knn1 <- knn(train = train_n, test = test_n, cl = train$Attrition, k=k1)
knn2 <- knn(train = train_n, test = test_n, cl = train$Attrition, k=k2)
knn3 <- knn(train = train_n, test = test_n, cl = train$Attrition, k=k3)
(knn1Acc <- 1- mean(knn1 != test$Attrition))
(knn2Acc <- 1- mean(knn2 != test$Attrition))
(knn3Acc <- 1- mean(knn3 != test$Attrition))
knn1Acc - accuracyB1 #same as benchmark
knn2Acc - accuracyB1 #worse then benchmark
knn3Acc - accuracyB1 #same as benchmark
dim(my_dataset_n)
my_dataset_c <- subset(my_dataset, select=c(2,3,5,7:9,11:15,20:22,25)) #isolate the categoricals
my_dataset_n <- cbind(my_dataset_n, my_dataset_c) #put them back together if you want to include the categoricals in a later question
logit <- glm(train$Attrition ~.,family=binomial(link='logit'),data=train)
summary(logit)
anova(logit, test="Chisq")
pR2(logit)
install.packages('pR2')
pR2(logit)
library(pscl)
install.packages('pscl')
install.packages("pscl")
pR2(logit)
library(pscl)
pR2(logit)
results.1.logit <- predict(logit,newdata=test[,-2],type='response')
results.1.logit <- ifelse(results.1.logit > 0.5,1,0)
(logitAcc1 <- 1- mean(results.1.logit != test$Attrition))
results.2.logit <- predict(logit,newdata=test[,-2],type='response')
results.2.logit <- ifelse(results.2.logit > 0.6,1,0)
(logitAcc2 <- 1- mean(results.2.logit != test$Attrition))
results.3.logit <- predict(logit,newdata=test[,-2],type='response')
results.3.logit <- ifelse(results.3.logit > 0.75,1,0)
(logitAcc3 <- 1- mean(results.3.logit != test$Attrition))
logitAcc1 - accuracyB1 #better than benchmark
logitAcc2 - accuracyB1 #better then benchmark
logitAcc3 - accuracyB1 #better (but not as much as above 2) as benchmark
#here we can reuse our approach to kNN somewhat -- k-means also needs normalised data
my_dataset_c <- subset(my_dataset, select=c(2,3,5,7:9,11:15,20:22,25)) #isolate the categoricals
my_dataset_k <- cbind(my_dataset_n, my_dataset_c) #put them back together
my_dataset_n <- subset(my_dataset, select=c(1,4,6,10,16:19,23,24,26:29))
kmeansClusters <- list()
kmeansScores <- c()
for (i in 2:10) {
clusters <- kmeans(my_dataset_n, i)
name <- paste("kmeans", i)
kmeansClusters[[name]] <- clusters
kmeansScores <- rbind(kmeansScores, sum(clusters$withinss))
}
row.names(kmeansScores) <- c(2:10)
colnames(kmeansScores) <- c("k")
plot(kmeansScores, xlab="k", ylab="within groups sum of squares")
library(clusterSim)
library(clusterSim)
for (i in 2:10) {
clusters <- kmeans(my_dataset_n, i)
name <- paste0("kmeans", i)
dbindex <- index.DB(my_dataset_n, clusters$cluster, centrotypes="centroids", p=2, q=2)
kmeansScores <- rbind(kmeansScores, dbindex$DB)
}
row.names(kmeansScores) <- c(2:10)
colnames(kmeansScores) <- c("k")
plot(kmeansScores, xlab="k", ylab="DBIndex")
#let's try plotting
install.packages("fpc")
library(fpc)
plotcluster(my_dataset_n, kmeansClusters[["kmeans2"]]$cluster)
plotcluster(my_dataset_n, kmeansClusters[["kmeans4"]]$cluster)
library(fpc)
plotcluster(my_dataset_n, kmeansClusters[["kmeans2"]]$cluster)
plotcluster(my_dataset_n, kmeansClusters[["kmeans2"]]$cluster)
library(fpc)
plotcluster(my_dataset_n, kmeansClusters[["kmeans2"]]$cluster)
setwd("C:/Users/jayan/Desktop/ADM/")
hrdata <-read.csv("ADM CA 1 Data.csv", stringsAsFactors = T) #will autoencode the text attributes to factors
#ok, now we need to make a dataset unique to you
set.seed(1337) # <-- put your student number here WITHOUT the x!! Leave off a starting zero if you have one
#e.g.: set.seed(62345678)
my_dataset <- hrdata[order(runif(600)), ]
#let's remove ID, we probably don't want that:
my_dataset <- my_dataset[-10]
#Unfortunately, due to a technical error, 3 columns of the data were lost :(
#HR blamed IT, IT blamed HR, your manager will blame you, so let's just hope those columns weren't important!
col1 <- round(runif(1)*32)+2 #the +2 protects the Age and Attrition Variables
col2 <- round(runif(1)*31)+2
col3 <- round(runif(1)*30)+2
cols <- names(my_dataset)
print(paste("I lost: ", cols[col1], ",", cols[col2], ",", cols[col3]))
#"I lost:  StandardHours , OverTime , Gender"
my_dataset <- my_dataset[-col1]
my_dataset <- my_dataset[-col2]
my_dataset <- my_dataset[-col3]
#if you want to use something other than R save your dataset:
write.csv(file="mydata.csv", my_dataset, row.names = F)
#Now please begin, and good luck!
#Because you lost 3 columns, some models may/may not work as well,
#don't worry about this, I will control for it in the grading!
##########################################
#Begin Foundations
#OK, so first off i lost some of my factor levels, and I'm lazy, so let's just read it in again
my_dataset <- read.csv("mydata.csv", stringsAsFactors = T)
str(my_dataset)
#let's deal with these other factors: F1
my_dataset$Education <- factor(my_dataset$Education, levels = c(1,2,3,4,5), labels=c("BC", "C", "UG", "MSc", "PhD"))
my_dataset$EnvironmentSatisfaction <- factor(my_dataset$EnvironmentSatisfaction, levels = c(1:4), labels=c("Low", "Medium", "High", "v High"))
my_dataset$JobInvolvement <- factor(my_dataset$JobInvolvement, levels = c(1:4), labels=c("Low", "Medium", "High", "v High"))
my_dataset$JobLevel <- factor(my_dataset$JobLevel) #insufficient information to do more
my_dataset$JobSatisfaction <- factor(my_dataset$JobSatisfaction, levels = c(1:4), labels=c("Low", "Medium", "High", "v High"))
my_dataset$PerformanceRating <- factor(my_dataset$PerformanceRating, levels = c(1:4), labels=c("Low", "Good", "Excellent", "Outstanding"))
my_dataset$RelationshipSatisfaction <- factor(my_dataset$RelationshipSatisfaction, levels = c(1:4), labels=c("Low", "Medium", "High", "v High"))
my_dataset$StockOptionLevel <- factor(my_dataset$StockOptionLevel) #don't have more information
my_dataset$WorkLifeBalance <- factor(my_dataset$WorkLifeBalance, levels = c(1:4), labels=c("Bad", "Good", "Better", "Best"))
#Now to fix factors where i don't have all levels
summary(my_dataset)
#I'm missing levels 0 and 1 in PerformanceRating, but the rest of my categoricals seem fine
my_dataset$PerformanceRating <- factor(my_dataset$PerformanceRating)
#however, as JobRole displays other, let's check more precisely
table(my_dataset$JobRole)
summary(my_dataset$JobRole)
#because i already encoded them above, all i have to do is reinvoke the factor command and my empty levels disappear
#a couple of the variables are pointless, so let's get rid of them
my_dataset <- my_dataset[,-9] #EmployeeCount -- always 1
my_dataset <- my_dataset[, -19] #over 18 -- always Y; strictly speaking we should confirm this, but under time pressure it's safe to assume this varibale isn't needed
#mising values:
sapply(my_dataset,function(x) sum(is.na(x)))
#outliers
boxplot(subset(my_dataset, select=c(1,6,18,20,21:24))) #nothing too obvious here
boxplot(subset(my_dataset, select=c(26:29))) #May be something in years at company
boxplot(my_dataset$DailyRate)
boxplot(my_dataset$HourlyRate)
boxplot(my_dataset$MonthlyIncome) #possibly some here too
boxplot(my_dataset$MonthlyRate)
#i prefer the attrition factor as 0/1 for convenience
my_dataset$Attrition <- factor(my_dataset$Attrition, labels=c(0,1), levels=c("No", "Yes"))
#F2: class balance:
table(my_dataset$Attrition) #fairly imbalanced
#F3: train and test
#2 options here stratified, or under/over sampling
library(caret)
sample <- createDataPartition(my_dataset$Attrition, p = .75, list = FALSE)
train <- my_dataset[sample, ]
test <- my_dataset[-sample, ]
#F3: train and test
#2 options here stratified, or under/over sampling
library(lattice)
#F4: benchmarks
#1 everyone stays:
str(test$Attrition) # remain (Y) is 0
b1 <- rep(0, dim(test)[1])
(accuracyB1 <- 1 - mean(b1 != test$Attrition))
#here we really see the effect of the class imbalance!
#2 those who have higher satisfaction are less likely to leave
str(my_dataset$JobSatisfaction)
b2 <- rep(0, dim(test)[1])
b2[test$JobSatisfaction == 'Low'] <- 1
b2[test$JobSatisfaction == 'Medium'] <- 1
(accuracyB2 <- 1 - mean(b2 != test$Attrition))
#End Foundations
##########################################
#Start of Question: B1
#work out what columns we have:
str(subset(my_dataset, select=c(1,4,6,10,16:19,23,24,26:29)))
#now correlation test
a <- cor(subset(my_dataset, select=c(1,4,6,10,16:19,23,24,26:29)))
view(a)
#End of Question
##########################################
#Start of Question: B2
boxplot(my_dataset$ï..Age~ my_dataset$Attrition)
spineplot(my_dataset$Education, my_dataset$Attrition)
#End of Question
##########################################
#Start of Question: B3
#I don't have gender, or overtime, so selecting marital status
spineplot(my_dataset$MaritalStatus, my_dataset$Attrition) #single people seem to leave more often
boxplot(my_dataset$DistanceFromHome ~ my_dataset$MaritalStatus) #surprisingly there doesn't appear to be much difference in commute distance by martial status
#End of Question
##########################################
#Start of Question: B4
plot(my_dataset$ï..Age, my_dataset$MonthlyRate, main = "Scatter plot of Age vs. Monthly Rate")
plot(my_dataset$ï..Age, my_dataset$DailyRate, main = "Scatter plot of Age vs. Daily Rate")
plot(my_dataset$ï..Age, my_dataset$HourlyRate, main = "Scatter plot of Age vs. Hourly Rate")
#doesn't look like it
#End of Question
##########################################
#Start of Question: B5
counts <- table(my_dataset$Attrition, my_dataset$JobSatisfaction)
row.names(counts) <- c("Remained", "Left")
barplot(counts, main="Attrition Distribution by Job Satisfaction", legend = row.names(counts))
#or if you want something a little easier to read
barplot(counts, main="Attrition Distribution by Job Satisfaction", xlab="Job Satistfaction", col=c("darkblue","red"), legend = rownames(counts), beside=T)
table(my_dataset$Attrition,my_dataset$JobSatisfaction)
#End of Question
##########################################
#Start of Question: I1
library(C50)
cFifty <- C5.0(Attrition ~ï..Age+Attrition+BusinessTravel+DailyRate+Department  , data=train, trials=100)
cFifty <- C5.0(Attrition ~., data=train, trials=100)
#if you saw: c50 code called exit with value 1 you had unused levels still in your factors or some single level factors (e.g. over 18)
str(my_dataset)
cFiftyPrediction <- predict(cFifty, newdata = test[, -2])
cFiftyAccuracy <- 1- mean(cFiftyPrediction != test$Attrition)
#0.8724832
cFiftyAccuracy - accuracyB1
#0.02684564 -- marginally better than always predicting remain
cFiftyAccuracy - accuracyB2
#0.2751678 -- a lot beter than just assuming that higher statisfaction are less likely to leave
#End of Question
##########################################
#Start of Question: I2
#kNN requires normalised data
library(class)
normalize <- function(x) { return ((x - min(x)) / (max(x) - min(x))) }
#now let's normalise our dataset so that calculating the distances in the feature space makes sense
my_dataset_n <- subset(my_dataset, select=c(1,4,6,10,16:19,23,24,26:29)) #get the numerical for normalisation -- kNN also doesn't support levelled factors either
my_dataset_n <- as.data.frame(lapply(my_dataset_n, normalize)) #normalise
summary(my_dataset_n) #all our numericals are normalised, our categoricals are untouched
?lapply
#re make train and test note we can retain the original distribution if we choose to
train_n <- my_dataset_n[sample, ]
test_n <- my_dataset_n[-sample, ]
#different ways to determine k
k1 <- round(sqrt(dim(train_n)[1])) #sqrt of number of instances
k2 <- round(sqrt(dim(train_n)[2])) #sqrt of number of attributes
k3 <- 7 #a number between 3 and 10
knn1 <- knn(train = train_n, test = test_n, cl = train$Attrition, k=k1)
knn2 <- knn(train = train_n, test = test_n, cl = train$Attrition, k=k2)
knn3 <- knn(train = train_n, test = test_n, cl = train$Attrition, k=k3)
(knn1Acc <- 1- mean(knn1 != test$Attrition))
(knn2Acc <- 1- mean(knn2 != test$Attrition))
(knn3Acc <- 1- mean(knn3 != test$Attrition))
knn1Acc - accuracyB1 #same as benchmark
knn2Acc - accuracyB1 #worse then benchmark
knn3Acc - accuracyB1 #same as benchmark
dim(my_dataset_n)
my_dataset_c <- subset(my_dataset, select=c(2,3,5,7:9,11:15,20:22,25)) #isolate the categoricals
my_dataset_n <- cbind(my_dataset_n, my_dataset_c) #put them back together if you want to include the categoricals in a later question
#don't run this line if you are going to do A1! PCA will break.
#End of Question
##########################################
#Start of Question: I3
logit <- glm(train$Attrition ~.,family=binomial(link='logit'),data=train)
summary(logit)
anova(logit, test="Chisq")
library(pscl)
install.packages("pscl")
pR2(logit)
results.1.logit <- predict(logit,newdata=test[,-2],type='response')
results.1.logit <- ifelse(results.1.logit > 0.5,1,0)
(logitAcc1 <- 1- mean(results.1.logit != test$Attrition))
results.2.logit <- predict(logit,newdata=test[,-2],type='response')
results.2.logit <- ifelse(results.2.logit > 0.6,1,0)
(logitAcc2 <- 1- mean(results.2.logit != test$Attrition))
results.3.logit <- predict(logit,newdata=test[,-2],type='response')
results.3.logit <- ifelse(results.3.logit > 0.75,1,0)
(logitAcc3 <- 1- mean(results.3.logit != test$Attrition))
logitAcc1 - accuracyB1 #better than benchmark
logitAcc2 - accuracyB1 #better then benchmark
logitAcc3 - accuracyB1 #better (but not as much as above 2) as benchmark
#Increasing the strictness of the model, seems to improve the performance, but it hasn't managed to outperform our benchmark yet
#End of Question
##########################################
#Start of Question: I4
#here we can reuse our approach to kNN somewhat -- k-means also needs normalised data
my_dataset_c <- subset(my_dataset, select=c(2,3,5,7:9,11:15,20:22,25)) #isolate the categoricals
my_dataset_k <- cbind(my_dataset_n, my_dataset_c) #put them back together
kmeansClusters <- list()
kmeansScores <- c()
for (i in 2:10) {
clusters <- kmeans(my_dataset_n, i)
name <- paste0("kmeans", i)
kmeansClusters[[name]] <- clusters
kmeansScores <- rbind(kmeansScores, sum(clusters$withinss))
}
row.names(kmeansScores) <- c(2:10)
colnames(kmeansScores) <- c("k")
plot(kmeansScores, xlab="k", ylab="within groups sum of squares")
#intuition would suggest 4 clusters... let's try another measure and find out
#i'd have been fine with up to here as an answer, but to improve our intutition of a possibly good value for k...
install.packages("pscl")
my_dataset_n <- subset(my_dataset, select=c(1,4,6,10,16:19,23,24,26:29))
kmeansClusters <- list()
kmeansScores <- c()
for (i in 2:10) {
clusters <- kmeans(my_dataset_n, i)
name <- paste0("kmeans", i)
kmeansClusters[[name]] <- clusters
kmeansScores <- rbind(kmeansScores, sum(clusters$withinss))
}
row.names(kmeansScores) <- c(2:10)
colnames(kmeansScores) <- c("k")
plot(kmeansScores, xlab="k", ylab="within groups sum of squares")
kmeansScores <- c()
for (i in 2:10) {
clusters <- kmeans(my_dataset_n, i)
name <- paste0("kmeans", i)
dbindex <- index.DB(my_dataset_n, clusters$cluster, centrotypes="centroids")
kmeansScores <- rbind(kmeansScores, dbindex$DB)
}
row.names(kmeansScores) <- c(2:10)
colnames(kmeansScores) <- c("k")
plot(kmeansScores, xlab="k", ylab="DBIndex")
#let's try plotting
install.packages("fpc")
install.packages("fpc")
library(fpc)
plotcluster(my_dataset_n, kmeansClusters[["kmeans2"]]$cluster)
plotcluster(my_dataset_n, kmeansClusters[["kmeans4"]]$cluster)
plotcluster(my_dataset_n, kmeansClusters[["kmeans5"]]$cluster)
plotcluster(my_dataset_n, kmeansClusters[["kmeans10"]]$cluster)
#for interest (going beyond what's asked, but it's good to think about this a little) let's use the whole dataset -- we need to switch to medoids because of the categorical variables
library(cluster)
kmedoidsClusters <- list()
kmedoidsScores <- c()
gower_dist <- daisy(my_dataset_k, metric = "gower", type = list(logratio = 3))
for (i in 2:20) { #for fun let's also increase the max k value as well
clusters <- pam(gower_dist, k=i, diss=T) #note we switched to the full set of attibutes now so need to use medoids
name <- paste0("kmedoids", i)
kmedoidsClusters[[name]] <- clusters
dbindex <- index.DB(gower_dist, clusters$clustering, centrotypes = "medoids", d=gower_dist)
kmedoidsScores <- rbind(kmedoidsScores, dbindex$DB)
}
row.names(kmedoidsScores) <- c(2:20)
colnames(kmedoidsScores) <- c("k")
plot(kmedoidsScores, xlab="k", ylab="DBIndex")
library(rpart)
library(rpart.plot)
library(rattle)
overfitModel <- rpart(Attrition ~ ., data=train, method="class", control=rpart.control(minsplit=2, cp=0))
notOverfitModel <- rpart(Attrition ~ ., data = train, method = "class")
#if you are willing to wait, we can plot
fancyRpartPlot(overfitModel)
fancyRpartPlot(notOverfitModel)
#or, we can just tap into some of the properites of the models
dim(overfitModel$splits)[1] #this is the no. of splits
dim(notOverfitModel$splits)[1] #this is the no. of splits
overfitP <- predict(overfitModel, test[,-2], type = "class")
(overFitAcc <- 1- mean(overfitP != test$Attrition))
overFitAcc - accuracyB1 #bad
notoverfitP <- predict(notOverfitModel, test[,-2], type = "class")
(notoverFitAcc <- 1- mean(notoverfitP != test$Attrition))
notoverFitAcc - accuracyB1 #less bad
notoverFitAcc - overFitAcc
#or, we can just tap into some of the properites of the models
dim(overfitModel$splits)[1] #this is the no. of splits
dim(notOverfitModel$splits)[1] #this is the no. of splits
overfitP <- predict(overfitModel, test[,-2], type = "class")
(overFitAcc <- 1- mean(overfitP != test$Attrition))
overFitAcc - accuracyB1 #bad
notoverfitP <- predict(notOverfitModel, test[,-2], type = "class")
(notoverFitAcc <- 1- mean(notoverfitP != test$Attrition))
notoverFitAcc - accuracyB1 #less bad
notoverFitAcc - overFitAcc
library(randomForest)
#Start of Question: I6
install.packages("randomForest")
install.packages("randomForest")
library(randomForest)
forest <- randomForest(Attrition ~ ., data=train, importance=TRUE, ntree=2000)
varImpPlot(forest)
#while we're at it, let's also see how well it does (for A3)
forestPrediction <- predict(forest, test[,-2], type = "class")
(forestAcc <- 1- mean(forestPrediction != test$Attrition))
#if you are willing to wait, we can plot
fancyRpartPlot(overfitModel)
